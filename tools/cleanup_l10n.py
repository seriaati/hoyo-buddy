from __future__ import annotations

import argparse
import ast
import logging
from pathlib import Path

import yaml

logger = logging.getLogger(__name__)

REPO_ROOT = Path(__file__).resolve().parents[1]
EN_US_PATH = REPO_ROOT / "l10n" / "en_US.yaml"


def iter_python_files(root: Path, include_tests: bool = True) -> list[Path]:
    files: list[Path] = []
    for p in root.rglob("*.py"):
        # Skip virtual envs, cache, migrations compiled cache, assets, bot data, etc.
        rel = p.relative_to(root).as_posix()
        if any(
            part in rel
            for part in (
                "__pycache__",
                ".venv/",
                "hoyo-buddy-assets/",
                "hoyo_buddy/bot/data/",
                "migrations/",
            )
        ):
            continue
        if not include_tests and (rel.startswith(("tests/", "test_"))):
            continue
        files.append(p)
    return files


def get_callee_name(node: ast.Call) -> str | None:
    f = node.func
    if isinstance(f, ast.Name):
        return f.id
    if isinstance(f, ast.Attribute):
        return f.attr
    return None


def _kw_to_dict(call: ast.Call) -> dict[str, ast.AST]:
    return {kw.arg: kw.value for kw in call.keywords if kw.arg is not None}


def _is_non_none(node: ast.AST | None) -> bool:
    if node is None:
        return False
    if isinstance(node, ast.Constant):
        return node.value is not None
    # For names/attributes, assume provided (non-None)
    return True


def collect_enum_keys(enums_py: Path) -> set[str]:
    """Collect all possible keys generated by EnumStr from StrEnum values.

    gen_string_key: replace(" ", "_").replace(",", "").replace(".", "").replace("-", "_").lower()
    """
    try:
        tree = ast.parse(enums_py.read_text(encoding="utf-8"), filename=str(enums_py))
    except Exception as e:
        logger.warning("Failed to parse enums at %s: %s", enums_py, e)
        return set()

    enum_values: list[str] = []

    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            # Check bases contain StrEnum
            is_str_enum = any(
                (
                    (isinstance(b, ast.Name) and b.id == "StrEnum")
                    or (isinstance(b, ast.Attribute) and b.attr == "StrEnum")
                )
                for b in node.bases
            )
            if not is_str_enum:
                continue
            for stmt in node.body:
                if isinstance(stmt, ast.Assign):
                    if len(stmt.targets) != 1:
                        continue
                    value = stmt.value
                    if isinstance(value, ast.Constant) and isinstance(value.value, str):
                        enum_values.append(value.value)

    def gen_string_key(s: str) -> str:
        return s.replace(" ", "_").replace(",", "").replace(".", "").replace("-", "_").lower()

    return {gen_string_key(v) for v in enum_values}


def collect_fstring_prefixes(root: Path, include_tests: bool = True) -> set[str]:
    """Detect f-string patterns like f'prefix.{var}' and return prefixes to preserve.

    Example: f"characters.sorter.{sorter.value}" -> preserve all "characters.sorter.*" keys
    """
    prefixes: set[str] = set()

    for file in iter_python_files(root, include_tests=include_tests):
        try:
            src = file.read_text(encoding="utf-8")
            tree = ast.parse(src, filename=str(file))
        except Exception as e:
            logger.warning("Skipping %s: %s", file, e)
            continue

        for node in ast.walk(tree):
            # Look for JoinedStr (f-string) nodes
            if not isinstance(node, ast.JoinedStr):
                continue

            # Reconstruct the f-string pattern - collect both prefix and suffix
            prefix_parts = []
            suffix_parts = []
            found_variable = False

            for value in node.values:
                if isinstance(value, ast.Constant) and isinstance(value.value, str):
                    if found_variable:
                        suffix_parts.append(value.value)
                    else:
                        prefix_parts.append(value.value)
                elif isinstance(value, ast.FormattedValue):
                    found_variable = True

            # Need at least a prefix or suffix with a variable
            if not found_variable or (not prefix_parts and not suffix_parts):
                continue

            # Handle dot-separated patterns like "characters.sorter.{var}"
            if prefix_parts:
                prefix = "".join(prefix_parts)
                if prefix.endswith("."):
                    stripped = prefix.rstrip(".")
                    if stripped and "." in stripped:
                        prefixes.add(stripped)
                elif prefix.endswith("_") and not suffix_parts:
                    # Simple underscore prefix without suffix: "prefix_{var}"
                    stripped = prefix.rstrip("_")
                    if stripped:
                        prefixes.add(stripped + "_")

            # Handle patterns like "prefix_{var}_suffix" -> match "prefix_*_suffix"
            if prefix_parts and suffix_parts:
                prefix = "".join(prefix_parts)
                suffix = "".join(suffix_parts)
                # For patterns like "shiyu_{index}_frontier", store as wildcard pattern
                if prefix and suffix:
                    # Store the full pattern with wildcard
                    prefixes.add(f"{prefix}*{suffix}")

    return prefixes


def collect_string_literals(
    root: Path, yaml_keys: set[str], include_tests: bool = True
) -> set[str]:
    """Scan for string literals that match YAML keys (e.g., in dicts/tuples)."""
    found: set[str] = set()

    for file in iter_python_files(root, include_tests=include_tests):
        try:
            src = file.read_text(encoding="utf-8")
            tree = ast.parse(src, filename=str(file))
        except Exception as e:
            logger.warning("Skipping %s: %s", file, e)
            continue

        for node in ast.walk(tree):
            # Check all string constants
            if (
                isinstance(node, ast.Constant)
                and isinstance(node.value, str)
                and node.value in yaml_keys
            ):
                found.add(node.value)

    return found


def collect_used_keys(root: Path, include_tests: bool = True) -> tuple[set[str], list[str]]:
    used: set[str] = set()

    saw_enumstr = False
    saw_weekdaystr = False
    saw_levelstr = False
    saw_timestr = False
    saw_unlocksin = False
    saw_raritystr = False

    for file in iter_python_files(root, include_tests=include_tests):
        try:
            src = file.read_text(encoding="utf-8")
            tree = ast.parse(src, filename=str(file))
        except Exception as e:
            # Log and skip unreadable/unparsable files (rare)
            logger.warning("Skipping %s: %s", file, e)
            continue

        for node in ast.walk(tree):
            if not isinstance(node, ast.Call):
                continue
            name = get_callee_name(node)
            if name is None:
                continue
            # LocaleStr(key="...")
            if name == "LocaleStr":
                kw = _kw_to_dict(node)
                key_node = kw.get("key")
                if isinstance(key_node, ast.Constant) and isinstance(key_node.value, str):
                    # Skip if mi18n or data_game specified
                    if (
                        _is_non_none(kw.get("mi18n_game"))
                        or _is_non_none(kw.get("data_game"))
                        or _is_non_none(kw.get("game"))
                    ):
                        continue
                    used.add(key_node.value)
                continue

            # app_commands.locale_str(..., key="...") or locale_str alias
            if name == "locale_str":
                kw = _kw_to_dict(node)
                key_node = kw.get("key")
                if isinstance(key_node, ast.Constant) and isinstance(key_node.value, str):
                    used.add(key_node.value)
                continue

            # Helper classes
            if name == "EnumStr":
                saw_enumstr = True
                continue
            if name == "WeekdayStr":
                saw_weekdaystr = True
                continue
            if name == "LevelStr":
                saw_levelstr = True
                continue
            if name == "TimeRemainingStr":
                saw_timestr = True
                continue
            if name == "UnlocksInStr":
                saw_unlocksin = True
                continue
            if name == "RarityStr":
                saw_raritystr = True
                continue

    if saw_weekdaystr:
        used.update(["monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday"])
    if saw_levelstr:
        used.add("level_str")
    if saw_timestr:
        used.add("time_remaining_str")
    if saw_unlocksin:
        used.add("unlocks_in_str")
    if saw_raritystr:
        used.add("rarity_str")

    if saw_enumstr:
        enum_keys = collect_enum_keys(root / "hoyo_buddy" / "enums.py")
        used.update(enum_keys)

    # Always preserve certain prefixes that are programmatically iterated
    # Translator.get_dyks() uses keys starting with "dyk_"
    preserve_prefixes = ["dyk_"]
    return used, preserve_prefixes


def load_yaml_keys(path: Path) -> dict[str, str]:
    with path.open("r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}
        if not isinstance(data, dict):
            msg = f"Unexpected YAML top-level type in {path!s}: {type(data)!r}"
            raise TypeError(msg)
        # Ensure all keys/values are strings (the file is a flat mapping)
        result: dict[str, str] = {}
        for k, v in data.items():
            if not isinstance(k, str):
                continue
            if isinstance(v, (str, int, float)):
                result[k] = str(v)
            elif v is None:
                result[k] = ""
            else:
                # For complex/multiline values, dump as string representation to avoid errors
                result[k] = str(v)
        return result


def _skip_multiline_content(lines: list[str], start_idx: int) -> int:
    """Skip multi-line YAML content and return the next line index."""
    i = start_idx
    while i < len(lines):
        next_line = lines[i]
        # Multi-line content is indented; stop when we hit a non-indented line
        if next_line.strip() and not next_line.startswith(" "):
            return i
        if next_line.strip().startswith("#"):
            return i
        i += 1
    return i


def _keep_multiline_content(lines: list[str], start_idx: int, output_lines: list[str]) -> int:
    """Keep multi-line YAML content and return the next line index."""
    i = start_idx
    while i < len(lines):
        next_line = lines[i]
        if next_line.strip() and not next_line.startswith(" "):
            return i
        if next_line.strip().startswith("#"):
            return i
        output_lines.append(next_line)
        i += 1
    return i


def write_yaml_keys(path: Path, data: dict[str, str]) -> None:
    """Remove keys by directly editing the file to preserve original formatting.

    This reads the file line by line and removes keys that are not in `data`,
    including handling multi-line YAML values (using | or > indicators).
    """
    lines = path.read_text(encoding="utf-8").splitlines(keepends=True)
    output_lines: list[str] = []
    keys_in_data = set(data.keys())

    i = 0
    while i < len(lines):
        line = lines[i]

        # Not a key-value line (comment, blank, etc.), always keep
        if ":" not in line or line.strip().startswith("#"):
            output_lines.append(line)
            i += 1
            continue

        # Extract the key (before the first colon, stripped)
        key_part = line.split(":", 1)[0].strip()
        value_part = line.split(":", 1)[1].strip()
        is_multiline = value_part in {"|", ">", "|-", ">-", "|+", ">+"}

        # If key should be kept, add it and handle multi-line if needed
        if key_part in keys_in_data:
            output_lines.append(line)
            if is_multiline:
                i = _keep_multiline_content(lines, i + 1, output_lines)
            else:
                i += 1
        elif is_multiline:
            # Skip this key and its multi-line content
            i = _skip_multiline_content(lines, i + 1)
        else:
            # Skip this single-line key
            i += 1

    # Write back to file
    path.write_text("".join(output_lines), encoding="utf-8")


def main() -> int:
    parser = argparse.ArgumentParser(description="Clean up unused l10n keys in en_US.yaml")
    parser.add_argument("--root", type=str, default=str(REPO_ROOT), help="Repository root to scan")
    parser.add_argument("--yaml", type=str, default=str(EN_US_PATH), help="Path to en_US.yaml")
    parser.add_argument("--write", action="store_true", help="Write changes back to the YAML file")
    parser.add_argument(
        "--include-tests", action="store_true", default=True, help="Include tests when scanning"
    )
    parser.add_argument(
        "--exclude-tests", action="store_true", help="Exclude tests when scanning (override)"
    )
    args = parser.parse_args()

    root = Path(args.root).resolve()
    yaml_path = Path(args.yaml).resolve()
    include_tests = args.include_tests and not args.exclude_tests

    keys_dict = load_yaml_keys(yaml_path)
    all_keys = list(keys_dict.keys())
    yaml_keys_set = set(all_keys)

    # Collect keys from LocaleStr/locale_str calls
    used, preserve_prefixes = collect_used_keys(root, include_tests=include_tests)

    # Collect keys from string literals that match YAML keys
    string_literal_keys = collect_string_literals(root, yaml_keys_set, include_tests=include_tests)
    used.update(string_literal_keys)

    # Collect f-string prefixes (e.g., f"characters.sorter.{var}" -> preserve "characters.sorter.*")
    fstring_prefixes = collect_fstring_prefixes(root, include_tests=include_tests)
    preserve_prefixes.extend(fstring_prefixes)

    # Hard-coded exclusions: keys that should never be marked as unused
    hard_exclusions = {"honkai:_star_rail", "none"}
    used.update(hard_exclusions)

    def is_preserved_by_prefix(k: str) -> bool:
        for pfx in preserve_prefixes:
            # Handle wildcard patterns like "shiyu_*_frontier"
            if "*" in pfx:
                prefix_part, suffix_part = pfx.split("*", 1)
                if k.startswith(prefix_part) and k.endswith(suffix_part):
                    return True
            elif k.startswith(pfx):
                return True
        return False

    # Separate keys into categories
    keys_in_yaml_and_used = len([k for k in all_keys if k in used])
    preserved_by_prefix = [k for k in all_keys if k not in used and is_preserved_by_prefix(k)]
    unused_keys = [k for k in all_keys if k not in used and not is_preserved_by_prefix(k)]

    print(
        f"en_US keys: {len(all_keys)} | "
        f"used: {keys_in_yaml_and_used} | "
        f"preserved by prefix: {len(preserved_by_prefix)} | "
        f"unused: {len(unused_keys)}"
    )

    if not unused_keys:
        print("No unused keys detected.")
        return 0

    # Show a short sample
    preview = 50
    print("Sample unused keys:")
    for k in unused_keys[:preview]:
        print(f"  - {k}")
    if len(unused_keys) > preview:
        print(f"  ... and {len(unused_keys) - preview} more")

    if not args.write:
        print("\nDry run (no changes written). Use --write to apply removals.")
        return 0

    # Write changes: remove keys while preserving order
    backup_path = yaml_path.with_suffix(yaml_path.suffix + ".bak")
    backup_path.write_text((yaml_path.read_text(encoding="utf-8")), encoding="utf-8")
    print(f"Backup written to: {backup_path}")

    new_map: dict[str, str] = {k: v for k, v in keys_dict.items() if k not in unused_keys}
    write_yaml_keys(yaml_path, new_map)
    print(f"Updated {yaml_path} (removed {len(unused_keys)} keys)")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
